---
layout: post
title: "ICLR 2020 NLP Paper Review"
subtitle: ICLR 2020 요약 정리
date: 2020-05-03
tags:
    - Paper Review
    - Machine Learning
---

ICLR 2020을 보고 NLP 분야에서 읽어봄직한 논문들을 제 맘대로 추려 간단하게 정리했습니다.
ICLR 2020 Virtual Site와 여러 큐레이션 블로그 글에서 관심 가는 것들을 기준으로 모아보았습니다.

중요하다고 생각하는 것들은 별도 글로 정리할 예정이며, 이 글에서는 개략적인 설명과 방법론, 이루어낸 결과물 정도로 정리합니다.
각 논문에는 OpenReview.net 상의 PDF가 링크되어 있습니다.

## 논문 목록

### ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators

-   [OpenReview.net 논문](https://openreview.net/pdf?id=r1xMH1BtvB)

기존의 Masked Language Modeling (MLM) 방식의 Pre-training 방법들은 `[MASK]` 토큰으로 기존 문장의 몇 개의 토큰을 치환해서 이를 추론하는 형태로 학습합니다.
Downstream Task로 내려가면 결과가 잘 나오지만, 많은 양의 Computing Resource를 필요로 합니다.

이 논문에서는 새로운 Pre-training 학습 방식인 **Token Detection**을 제안합니다.
입력에 있는 몇 개의 토큰을 Masking하는 대신, 작은 Generator Network로부터 **그럴듯한 대체 토큰을 하나 생성**해내서 치환합니다.
우리가 학습시키는 모델은 각 토큰이 Generator에서 나온 토큰인지 아닌지를 구별하는 형태로 학습을 진행합니다.
이 방식은 오직 몇 개의 Masking된 토큰에 대해서 판단하는 대신 존재하는 모든 토큰에 대해 판단하기 때문에 기존 방식보다 효율적으로 학습할 수 있습니다.

실험 결과 **같은 조건의 BERT를 Outperform**하는 결과를 냈고, 작은 모델일수록 특히 효과가 좋았습니다.
GPU 하나를 가지고 4일 간 학습한 모델이 GLUE 벤치마크를 기준으로 GPT를 뛰어넘었습니다.
Scaling을 기준으로 보아도 효과가 있었습니다.
약 4분의 1 정도의 Resource를 사용했음에도 RoBERTa와 XLNet과 비교할 만한 성능을 내었고, 비슷한 수준을 사용했을 경우 이들보다 나은 성능을 냈습니다.

### Large Batch Optimization for Deep Learning: Training BERT in 76 Minutes

-   [OpenReview.net 논문](http://www.openreview.net/pdf?id=Syx4wnEtvH)

큰 크기의 Dataset을 가지고 크고 깊은 Neural Network를 학습시키는 것은 연산량 측면에서 굉장히 어려운 일입니다.
이 문제를 해결하기 위해 최근 큰 크기의 Batch에 대해 Stochastic Optimization을 적용하는 방안에 대한 관심이 급증했습니다.
이 분야에 대한 연구로 [LARS (Yang You et al. 2017)](https://arxiv.org/abs/1708.03888)가 있었고, Layer-wise로 Adaptive Learning Rate를 적용해서 몇 분 이내에 ResNet이나 ImageNet을 훈련시킬 수 있는 알고리즘입니다.
하지만 LARS는 BERT와 같은 Attention 기반의 모델에는 성능이 좋지 않았고, Downstream Task들에 대해 성능 상의 이득이 일정하지 않았습니다.

이 논문에서는 큰 크기의 Mini-batch를 활용해서 Layer-wise Adaptation을 하는 전략으로 연구를 진행했고, **LAMB**라는 이름의 Large Batch Optimization Technique를 제시하고 정상적으로 수렴함을 보였습니다.

실험 결과 BERT나 ResNet-50에 대해서 매우 작은 Hyperparameter Tuning을 통해 더 나은 성능을 보였습니다.
특히, BERT를 학습시킬 때 **32,868개의 샘플을 한 배치로 묶었음에도 성능 저하가 일어나지 않았고** 3일 정도의 학습 뒤에 원하는 결과에 도달했습니다.
GCP의 TPU v3 Pod를 사용해서 Memory Limit을 올렸을 때, BERT의 학습 시간이 3일에서 고작 **76분**으로 줄었습니다.

### Pre-training Tasks for Embedding-based Large-scale Retrieval

-   [OpenReview.net 논문](http://www.openreview.net/pdf?id=rkg-mA4FDr)

이 논문은 Large-scale Query-Document Retrieval Problem을 다루었습니다.
이 문제는 큰 규모의 Document Corpus에서 특정 Query가 주어졌을 때 관련된 Document를 찾아내는 형태인데, 보통 다음의 두 과정을 통해 해결합니다.

-   Retrieval Phase
    -   Solution Space를 줄이고, 후보 Document들에 대한 Subset을 찾음
-   Scoring Phase
    -   Document를 Reranking함

이 알고리즘은 높은 Recall을 요하지만, 동시에 Sublinear한 수준의 시간 복잡도로 후보 Document들을 불러올 수 있을 정도로 효율적이어야 합니다.
Scoring Phase는 BERT와 같은 형태의 Pretraining 단계를 가지는 여러 연구들에 의해 충분히 연구되었지만, Retrieval Phase는 BM25와 같은 Information Retrieval 방법들만을 사용했을 뿐 그 정도까지 깊게 연구되지 않았습니다.
이런 형태의 모델들은 희소한 Feature들에 대해 효과가 있으며 다양한 Downstream Task들에 대해서 최적화할 수 없습니다.

이 논문에서는 Embedding 기반의 Retrieval Model에 대한 포괄적인 연구를 진행했습니다.
그 결과 강력한 Embedding 기반 Transformer 모델의 필수 요소는 **잘 구성된 Pre-training Task들**이라는 결론을 보입니다.
적절하게 설계된 Paragraph-level Pretraining Task를 사용한 Transformer 모델은 BM-25 자체와 Transformer를 사용하지 않은 Embedding Model의 성능을 각각 월등히 넘어섰습니다.

### ALBERT: A Lite BERT for Self-supervised Learning of Language Representations

-   [OpenReview.net 논문](http://www.openreview.net/pdf?id=H1eA7AEtvS)

Pretraining 단계에서 Model의 크기를 키우면 Downstream Task에서의 성능 향상을 이루어내는 경우가 많습니다.
그러나, GPU나 TPU에서의 메모리 제약이나 길어지는 학습 시간 때문에 점점 어려워지고 있습니다.

이 논문에서는 메모리 소모를 줄이고 더 빠르게 학습을 진행하기 위한 다음의 두 가지 Parameter-reduction Technique를 제시합니다.

-   **Factorized Embedding Parameterization**
    -   BERT에서는 Input Token의 Embedding Size(E)와 Hidden Size(H)가 같음
    -   E를 H보다 작게 설정하여 성능 손실이 거의 없이 모델의 크기를 줄일 수 있음
-   **Cross-layer Parameter Sharing**
    -   여러 Transformer Layer 사이에 Parameter를 공유함
    -   Self Attention Layer는 성능 손실 적음, FFN은 성능 손실 있음
    -   결국 Parameter의 갯수가 적어지므로 모델의 크기를 줄일 수 있음

이 두 내용을 바탕으로 문장 간의 Coherence를 모델링하는 형태의 Self-supervised Loss를 제안했고, 여러 문장을 입력으로 놓는 Downstream Task들에게 기여함을 보였습니다.
BERT Large보다 적은 Parameter들을 가졌음에도 불구하고 GLUE와 RACE, SQuAD 벤치마크에서 SOTA를 달성했습니다. 코드는 [여기](https://github.com/google-research/ALBERT)에 공개되어 있습니다.

### BERTScore: Evaluating Text Generation with BERT

-   [OpenReview.net 논문](http://www.openreview.net/pdf?id=SkeHuCVFDr)

이 논문에서는 Text Generation을 위한 자동 Evaluation Metric인 `BERTScore`를 제시합니다.
기존의 Metric들과 유사하게, Reference Sentence와 Candidate Sentence 간의 Similarity Score를 계산하는 것은 동일합니다.
그러나, Exact Match를 보는 것이 아닌 **Token Similarity를 계산**하고, 이를 위해 **Contextual Embedding**을 사용합니다.

363개의 기계 번역 및 Image Captioning 시스템을 사용하여 Evaluation을 해보았고, 그 결과 **기존보다 Human Judgement와 부합하는 결과**가 나옴과 동시에 더욱 강력한 모델이 유도되었습니다.
Adversarial Paraphrase Detection Task를 BERTScore를 통해 학습시켰으며, 어려운 Example들에 대해 다른 기존의 Metric들보다 Robust한 결과를 얻었습니다.

### StructBERT: Incorporating Langauge Structures into Pre-training for Deep Language Understanding

-   [OpenReview.net 논문](http://www.openreview.net/pdf?id=BJgQ4lSFPH)

최근 Pretraining된 LM인 BERT가 NLU 분야에서 큰 주목을 받았고, 여러 NLU Task에서 SOTA에 해당하는 Accuracy를 달성했습니다. 이 논문에서는 Language Structure를 함께 사용한 BERT의 확장인 `StructBERT`를 제시합니다.

이들은 단어와 문장 레벨에서의 Language Structure를 활용하는 형태의 두 가지 보조 Task를 이용해서 StructBERT를 Pretrain했습니다.

-   Word Structural Objective
    -   **Structure상 순서가 섞인 Token**을 Input으로 넣음
    -   각 Position에 맞는 Token이 나왔는지를 Loss로 사용
-   Sentence Structural Objective
    -   Sentence의 Sequential Order를 예측하도록 설정
    -   NSP 뿐만이 아니라 **Previous, Random Sentence Prediction**까지 사용
    -   각각 3분의 1 확률로 사용

Structural Pretraining을 사용해서 훈련한 결과 예상을 뛰어넘는 수준의 좋은 결과를 얻을 수 있었습니다.
GLUE 벤치마크의 SOTA를 89.0으로 재갱신하였고, 이것은 해당 Model Submission 시점에서 가장 좋은 점수였습니다.
이 뿐만 아니라 SQuAD v1.1에서 F1 Score 93.0, SNLI Accuracy 91.7을 달성하였습니다.

### Plug and Play Language Models: A Simple Approach to Controlled Text Generation

-   [OpenReview.net 논문](http://www.openreview.net/pdf?id=H1edEyBKDS)

Transformer에 기반을 두고 엄청난 규모의 말뭉치들을 바탕으로 학습된 LM들은 이전에 없었던 수준의 Generation 능력을 보여주었습니다.
그러나, 주제나 감정을 바꾸는 것처럼 Generation된 결과 자체의 Attribute들을 제어하는 것들은 모델의 설계 구조를 변경하거나 Fine-tuning을 하지 않고는 굉장히 힘듭니다.
애초에 그렇게 하는 것도 많은 리소스를 요구하는 힘든 작업입니다.

이 논문에서는 제어 가능한 Language Generation을 위해서 **Plug and Play Language Model(PPLM)**을 제시합니다.
이 모델은 Pretrained LM에 하나 혹은 그 이상의 Attribute Classifier들을 합친 형태로 구성되어, LM에 추가적인 학습을 시킬 필요 없이 Generation에 영향을 주도록 설계되었습니다.
Attribute Classifier들은 간단한 BoW나 Layer 하나 정도 되는 수준의 모델의 집합으로, 여기서 사용되는 Layer는 실제 LM보다 100,000배 정도 적은 양의 Parameter로 학습됩니다.
각 Classifier들은 다음과 같은 과정으로 LM의 결과에 직접적인 영향을 주게 됩니다.

1. **Forward Pass**

-   LM의 Original Distribution이 나옴
-   Attribute Model은 위 값으로부터 목표한 Attribute의 Likelihood를 추론함

2. **Backward Pass**

-   LM의 기존 Latent Representation에 Attribute Model의 Gradient를 흘림
-   LM의 Output이 목표하는 Attribute의 값을 갖도록 Likelihood를 조정함

3. **Recompute**

-   다시 해당 LM에 대한 Forward 실행
-   원하는 Attribute가 반영된 결과를 얻음

실험에서는 다양한 Topic이나 Sentiment Style에 대한 영향을 보였으며, 사람의 평가 및 자동화된 평가 각각에서 Attribute Alignment와 Fluency를 보였습니다.

꽤나 신기한 결과를 보여 여기에도 하나 첨부합니다. 아래는 논문에서 제시한 Sentimental Control의 사례입니다. 내용은 전혀 유지되지 않지만, 문장에 드러나는 분위기 자체는 단어들에 의해 크게 제어되고 있고 문장 구조도 명확하게 떨어집니다.

> **[Baseline GPT-2]** The country’s new chief minister, A.J. Paik, is a member of a group of prominent conservative politicians who have criticized the Obama administration’s efforts to...<br> > **[PPLM-Positive]** The country’s largest indoor painting event! Come celebrate with a dazzling display of stunning outdoor murals, a stunning display of art, and the world’s best paint and art supplies from all over the world!<br> > **[PPLM-Negative]** The country’s top prison system is forcing prisoners to use a trash dump, rather than a toilet, to flush their waste out, as the authorities fear the waste is more toxic and could cause cancer, an official at a major prison has revealed...

### Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model

-   [OpenReview.net 논문](http://www.openreview.net/pdf?id=BJlzm64tDH)

최근 Pretrained Language Model들이 보여주는 성능은 Self-supervised Learning이 NLP에 가지는 효율성을 보였습니다.
통사적 구조나 의미적 구조도 있지만, Real-world Knowledge 쪽에서도 큰 발전이 있었습니다.
큰 규모로 LM을 잡는 방법이 Knowledge Capturing을 암시적으로 수행하는 방법으로 여겨지는 경우도 있었습니다.

이 논문에서는 Zero-shot Fact Completion Task를 확장하여 BERT와 같은 Pretrain된 모델들이 어느 정도로 Knowledge를 Capturing하는지 조사했습니다.
또한 실제 존재하는 Entity들과 Knowledge를 연관짓도록 하는 간단한 Weakly Supervised Pretaining Objective를 다음과 같이 제시했습니다.

1. 원본 Document를 받아 각 Entity를 인식하고 Wikipedia Entity와 연관 지음
2. 이를 Positive로 보고, Negative Statement를 만듬

-   몇몇 Entity들을 **같은 범주의 다른 Random Entity로** 바꿈
-   Linguistic Correctness의 유지를 위함

3. 각 Entity에 대하여, **이 Entity가 치환된 것인지 아닌지**를 Binary Classification함

일반적인 LM Objective와 비교했을 때, 이 방법은 Replacement를 Entity Level에서 실행한다는 점과, Negative Signal이 더욱 강력하다는 점에서 차이가 있습니다.

이 방법으로 훈련한 모델들은 Fact Completion Task에서 큰 성능 향상을 보였습니다.
Downstream Task에 적용하였을 때, 4개의 Entity-related Question Answering Dataset에서 BERT를 넘어서는 성능을 보였습니다. (F1 Score 평균 2.7 향상, Entity Typing Dataset 기준 Accuracy 5.7 향상)

### LAMOL: LAnguage MOdeling for Lifelong Language Learning

지금까지 있었던 Lifelong Learning에 대한 연구는 대부분 이미지나 게임을 대상으로 했었고, 언어에는 관련한 연구가 없었습니다. 이 논문에서는 간단하고 효과적인 Lifelong Language Learning(LLL) 방법론인 LAMOL을 제안합니다.

LAMOL은 이전의 Task부터 나온 Pseudo-sample들을 추가 메모리나 모델 크기에 구애받지 않는 선에서 계속 재학습하는 형태로 이루어집니다.
조금 더 자세하게는 **Task를 해결하는 방법과 Training Sample을 만드는 방법을 동시에 학습**한다고 볼 수 있습니다.
모델이 새 Task에 대해 학습될 때, 새 Task의 Alongside Data를 학습시키려는 목적으로 Pseudo-sample들을 만들어냅니다.

이러한 방법은 아무런 기억 공간의 타협없이 **Catastrophic Forgetting을 방지**하는 효과가 있었으며, 아주 다른 5개의 언어 관련 Task를 하나의 모델로 순차적으로 처리할 수 있었습니다.
이전의 방법들을 상당한 차이로 앞질렀으며, Multitasking 방식과 비교했을 때 LLL 분야에서 용인 가능한 수준인 2~3%의 성능 손실을 기록했습니다. 소스 코드는 [여기](https://github.com/jojotenya/LAMOL)에서 찾아볼 수 있습니다.

### The Curious Case of Neural Text Degeneration

-   [OpenReview.net 논문](http://www.openreview.net/pdf?id=rygGQyrFvH)

Neural Language Modeling 분야에 있었던 큰 발전에도 불구하고, Text Generation 과정에서 사용하는 최고의 Decoding Strategy는 무엇인가에 대한 질문은 아직 해결되지 않았습니다.
비직관적이지만 실험적으로 나온 결과는, Likelihood를 사용하는 방법이 NLU 분야에서 높은 성능의 결과를 나타내었음에도 불구하고, Beam Search와 같은 Maximization에 기반을 둔 방법들이 되려 성능을 퇴화시키는 것이었습니다.
그 결과로 나오는 것들은 의미가 불분명하거나, 논리적인 맥락이 없거나, 루프에 갇히는 등 그닥 좋은 성능을 보이지는 않았습니다.

이 논문에서는 이 문제를 해결하기 위해 **Nucleus Sampling**이라는 새로운 Decoding Strategy를 제안합니다.
이 방법은 전체 확률 분포에서 의미가 크지 않은 꼬리 부분을 사용하지 않고, **큰 확률 질량이 관측되는 부분에서 샘플링**하는 형태를 사용함으로써 성능을 높였습니다.

이 방법을 실험하기 위해 Nucleus Sampling 방식으로 Decode된 문장과 사람이 만든 문장을 Likelihood, Diversity, 혹은 Repetition과 같은 다양한 기준을 바탕으로 비교했습니다. 그 결과 이 논문에서 제시하는 결론은 다음의 세 가지입니다.

-   Maximization 방식은 Open-ended Text Generation에 맞지 않다
-   현재 최고의 성능을 가지는 LM에는 신뢰하지 못하는 확률 분포 상의 구간이 있다
    -   그러므로 실제 Generation 과정에서 잘려나가야 한다
-   Nucleus Sampling이 길고 높은 품질의 문장을 생성하는 데에 가장 적합한 Decoding Strategy에 해당한다

### Thieves on Sesame Street! Model Extraction of BERT-Based APIs

-   [OpenReview.net 논문](http://www.openreview.net/pdf?id=Byl5NREFDr)

이 논문에서는 기존에 Image나 Vision 분야에 존재하던 Model Extraction 문제를 NLP 분야에서 연구했습니다.
연구 대상 형태는 공격자가 피해 모델에 가지는 **Query Access만으로 해당 Model의 Local Copy를 취득**할 수 있는 형태입니다.

만약 공격자의 모델과 피해 모델이 전부 BERT와 같은 Pretrained Language Model을 Fine-tune한 형태라고 가정하겠습니다.
논문에 따르면, 공격자는 실제 Training Data나 문법적/의미적으로 가치 있는 Query를 사용할 필요도 없이 공격을 성공할 수 있습니다.
특정 작업의 Heuristic이 조금 섞인 Random Word Sequence(Rubbish Input)와, 이를 입력으로 넣어 나온 응답만을 가지고 다양한 NLP Task에서 효과적으로 모델을 추출해낼 수 있습니다.

이 논문은 위와 같은 형태를 지니는, 수백 달러 정도의 예산만 있으면 피해 모델보다 아주 약간 성능이 덜 나오는 수준의 모델을 얻을 수 있는 Exploit을 제시합니다.
또한, 다음의 두 가지 방어 전략도 함께 제시합니다.

-   Membership Classification
    -   Outlier Detection; Nonsensical하거나 Ungrammatical한 Input을 탐지함
    -   공격자가 실제 Query를 쉽게 취득할 수 없는 환경에서만 사용 (정상 OOD 데이터 위험)
-   API Watermarking
    -   작은 범위의 Query들에 대해 오답 Output을 내보냄
    -   실제 Model 추론 과정을 타지 않고 API 단에서 처리함
    -   실제 문제 해결이 안되고 탐지만 된다는 것과 Watermark 사실을 공격자가 아는 경우 역조치가 취해질 수 있음

## 참조

-   [ICLR 2020 - OpenReview.net](https://openreview.net/group?id=ICLR.cc/2020/Conference)
-   [Google at ICLR 2020 - Google AI Blog](https://ai.googleblog.com/2020/04/google-at-iclr-2020.html)
-   [Best NLP Papers from ICLR 2020 - Christina Kim's Blog](https://christina.kim/2020/04/23/the-nlp-papers-to-read-before-iclr-2020/)
-   [Yang You et al. 2017 (LARS) - ArXiv](https://arxiv.org/abs/1708.03888)
-   [google-research/ALBERT - GitHub](https://github.com/google-research/ALBERT)
-   [jojotenya/LAMOL - Github](https://github.com/jojotenya/LAMOL)
